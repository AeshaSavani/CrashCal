{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138ed38e-4be2-4ef5-80c4-25930033f966",
   "metadata": {},
   "source": [
    "__CRASHCAL__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd5856-50c0-408c-87c5-b42986e62f72",
   "metadata": {},
   "source": [
    "CrashCal is an AI-powered solution designed to automate car damage assessment by analyzing vehicle images. It detects whether a car is damaged, identifies the damaged part, estimates the severity of the damage, and predicts the repair cost. This tool aims to streamline the insurance claims and repair estimation process with speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2d62e-ed6d-4b8b-9bb0-8c143ce2cb2a",
   "metadata": {},
   "source": [
    "Step 1: Car Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7ef81-d50f-4f84-865b-f0bdab454036",
   "metadata": {},
   "source": [
    "Step 2: Damage Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901f18f-6c41-46d8-b8b3-09a0eb0874cd",
   "metadata": {},
   "source": [
    "Step 3: Damage Localization (Which Part is Damaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80ad64-8e7c-420c-a68b-1d43196a5d80",
   "metadata": {},
   "source": [
    "Step 4: Damage Severity Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a917ab59-27be-49ba-83db-1e684214f132",
   "metadata": {},
   "source": [
    "Step 5: Repair Cost Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f418bff7-aeaf-4232-8f50-0607c61f0561",
   "metadata": {},
   "source": [
    "Step 6: Integration and Frontend Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8cd77-21ff-4bb5-b8ea-4af0583f480f",
   "metadata": {},
   "source": [
    "![flowchart](flowchart.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32bd8b-1ba0-40d6-b466-8fa6a6816d62",
   "metadata": {},
   "source": [
    "__STEP 1: CAR DETECTION__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d50dd-6464-42f8-a712-1d74eb44416c",
   "metadata": {},
   "source": [
    "1. Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fada17fe-e90e-4685-8f82-f65df2d2982a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, load_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from PIL import UnidentifiedImageError, Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163de5c0-a843-4199-b77d-4eb6246d5adb",
   "metadata": {},
   "source": [
    "2. Data Cleaning\n",
    "   \n",
    "   a). Iterates through images in specified folders, \n",
    "\n",
    "   b). verifies if they are corrupted, \n",
    "\n",
    "   c). and removes any corrupted images it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045deab-77ab-41f1-ab00-1f7f460b72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corrupted_images(data_dir):\n",
    "    for folder in ['car', 'not_car']:\n",
    "        folder_path = os.path.join(data_dir, folder)\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img.verify()  # Check if image is corrupted\n",
    "            except (IOError, UnidentifiedImageError):\n",
    "                print(f\"Removing corrupted image: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "\n",
    "# Call this function on your dataset folder\n",
    "data_dir = r\"E:\\car_crash\\cars\"\n",
    "remove_corrupted_images(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7359c-97dd-4d22-9848-0359ac07655e",
   "metadata": {},
   "source": [
    "This function attempts to load and preprocess an image, returning it as a normalized array, and skips the image if itâ€™s corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de1cb2-c6c6-4fc0-bc80-fecee1d12848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_safe(image_path):\n",
    "    try:\n",
    "        img = image.load_img(image_path, target_size=(150, 150))\n",
    "        return image.img_to_array(img) / 255.0\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"Warning: Skipping corrupted image {image_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e05ea-a986-4baa-bd37-8de1f9835d5b",
   "metadata": {},
   "source": [
    "Generates batches of preprocessed images and their corresponding labels for training a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ed9dc-25fc-47df-a7da-96bf2e149916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, file_paths, labels, batch_size, img_size=(150, 150)):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_paths = self.file_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for i, path in enumerate(batch_paths):\n",
    "            img = load_image_safe(path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(batch_labels[i])\n",
    "        \n",
    "        return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81652da-5960-4561-b0aa-39d05728b98c",
   "metadata": {},
   "source": [
    "3. Data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac61fbb-e26f-47f6-aaeb-8abcd709ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get image paths and labels\n",
    "car_paths = [os.path.join(data_dir, 'car', img) for img in os.listdir(os.path.join(data_dir, 'car'))]\n",
    "not_car_paths = [os.path.join(data_dir, 'not_car', img) for img in os.listdir(os.path.join(data_dir, 'not_car'))]\n",
    "\n",
    "all_image_paths = car_paths + not_car_paths\n",
    "labels = [1] * len(car_paths) + [0] * len(not_car_paths)  # 1 for car, 0 for not_car\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(all_image_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create generators\n",
    "train_generator = CustomDataGenerator(train_image_paths, train_labels, batch_size=250)\n",
    "validation_generator = CustomDataGenerator(val_image_paths, val_labels, batch_size=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d516c-f39f-4764-8c24-9a7e64ce8f9b",
   "metadata": {},
   "source": [
    "4. CNN model\n",
    "   \n",
    "   a) using three convolutional layers with ReLU activation & passes it through fully connected layers for binary classification with a sigmoid output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764e2b0-6e6c-4234-902b-182dee31b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5f1bc-8d00-4051-97ae-3ff2b3fae4c9",
   "metadata": {},
   "source": [
    "5. Model training \n",
    "\n",
    "   a) trains CNN model on training dataset\n",
    "   \n",
    "   b) tests the model on validation dataset\n",
    "   \n",
    "   c) runs for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30c047-8770-48bb-9d12-f0161f9cf6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a5ddb-8d3d-45e5-b315-ff01de2cf96f",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b89f2-fa81-4d2c-81d9-89a01f19230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation data\n",
    "val_loss, val_acc = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05d29a-036b-4b37-9dfe-964bea1556fa",
   "metadata": {},
   "source": [
    "6. Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e836a7-ebc7-4085-a20f-6ec157596b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in HDF5 format\n",
    "model.save('car_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d57f0-9123-4e30-aefc-4e55f6e00251",
   "metadata": {},
   "source": [
    "7. Data visualization\n",
    "\n",
    "   a) plot for training vs validation accuracy and loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f412f-8d16-4f9d-a07d-384fdfd7d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(5)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc020d2-4925-4e4c-8189-c80d40d5a0ae",
   "metadata": {},
   "source": [
    "b) Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b32c91-ed7e-43bc-b442-eed1c43b768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "Y_pred = model.predict(validation_generator)\n",
    "y_pred = (Y_pred > 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(val_labels[:len(y_pred)], y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Car', 'Car'], yticklabels=['Not Car', 'Car'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41345e4b-b86b-4547-b3ef-9fa0c297871b",
   "metadata": {},
   "source": [
    "8. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e793c64-e42d-419e-a555-85f4d5aa024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(150, 150))  # Resize image\n",
    "    img_array = image.img_to_array(img) / 255.0  # Normalize pixel values\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# Function to visualize prediction\n",
    "def visualize_prediction(image_path, model):\n",
    "    img_array = load_image(image_path)\n",
    "    prediction = model.predict(img_array)[0][0]  # Predict using the trained model\n",
    "    \n",
    "    # Assign label based on prediction threshold (0.5)\n",
    "    label = \"Car\" if prediction >= 0.5 else \"Not Car\"\n",
    "    \n",
    "    # Load original image for display\n",
    "    original_image = image.load_img(image_path)\n",
    "    \n",
    "    # Plot the image with the predicted label\n",
    "    plt.imshow(original_image)\n",
    "    plt.title(f\"Prediction: {label} ({prediction:.2f})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Test with an example image from your dataset\n",
    "test_image_path = r\"E:\\car_crash\\test\\test1.jpg\"  # Path to test image\n",
    "visualize_prediction(test_image_path, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662a6e4-9b1a-424a-be81-8fc774b1fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, img_path):\n",
    "    img = image.load_img(img_path, target_size=(150, 150))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    if prediction < 0.8:\n",
    "        print(\"The image is predicted to be: Not a Car\")\n",
    "    else:\n",
    "        print(\"The image is predicted to be: A Car\")\n",
    "\n",
    "# Test with a random image\n",
    "img_path = r\"E:\\car_crash\\test\\test1.jpg\"  # Image path\n",
    "predict_image(model, img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cee0e-beb5-4cbb-a878-c6fb7228ba94",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e82230-eaa4-4b5e-81a9-e8cd55435d31",
   "metadata": {},
   "source": [
    "__STEP 2: DAMAGE DETECTION__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20041a7-edf6-4bdc-91d3-f57728444f04",
   "metadata": {},
   "source": [
    "1. Importing libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc176714-406c-43c5-84a8-3a0e8b85b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bc55b-90b0-48bb-8d4a-7e4cbf875646",
   "metadata": {},
   "source": [
    "2. Loading training & validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23554a-16e5-4cab-a52a-3a76a99a353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "train_dir = r\"E:\\car_crash\\damage\\training\"\n",
    "val_dir = r\"E:\\car_crash\\damage\\validation\"\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049b278-df4c-4f61-9392-4f624c9e25de",
   "metadata": {},
   "source": [
    "3. Data(image) preprocessing \n",
    "\n",
    "   a) creates data generators to preprocess images by rescaling pixel values , resizing them to the IMG_SIZE and assigning binary labels for classification.\n",
    "\n",
    "   b) prepares the data for a binary classification model, handling image loading, resizing, and normalization in batches (BATCH_SIZE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbc5df-c236-451a-a3e6-ff20792d0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09385a-2561-47b3-aaa7-4eb9df030558",
   "metadata": {},
   "source": [
    "4. CNN model\n",
    "   \n",
    "   a) using three convolutional layers with ReLU activation & passes it through fully connected layers for binary classification with a sigmoid output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcc9b6-d196-4c03-8129-c603baed052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8a147-f84a-4fff-85bf-5a4f5b87eaf2",
   "metadata": {},
   "source": [
    "5. Model training \n",
    "\n",
    "   a) trains CNN model on training dataset\n",
    "   \n",
    "   b) tests the model on validation dataset\n",
    "   \n",
    "   c) runs for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e76bd-3607-409f-99d2-06263d4fc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,validation_data=val_generator,epochs=10  # Adjust based on performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ef812-c33e-4d41-88f8-2f27c4d379c0",
   "metadata": {},
   "source": [
    "6. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a5a9c-bb09-42cd-9624-d1c9a21d5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation data\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186e207-6675-42af-9817-a5d98e031bf4",
   "metadata": {},
   "source": [
    "7. Data visualization\n",
    "\n",
    "   a) plot for training vs validation accuracy and loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb805b3d-04ba-4902-8c39-e498e98c8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e8836-657e-4421-9ed4-a3bb4f51a0eb",
   "metadata": {},
   "source": [
    "b) Predicted vs Actual Results on Sample Validation Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6380747-7742-4a85-ada9-9a27ea2c0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(generator, num_images=5):\n",
    "    # Get a batch of images and labels\n",
    "    images, labels = next(generator)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = model.predict(images)\n",
    "\n",
    "    # Plot images with predictions\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i+1)\n",
    "        plt.imshow(images[i])\n",
    "        # Show label and prediction\n",
    "        plt.title(f\"Label: {int(labels[i])}\\nPred: {'Not Damaged' if predictions[i] > 0.4 else 'Damaged'}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions on validation data\n",
    "visualize_predictions(val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4846bd3-c305-4b0c-9435-5674a37fd8fb",
   "metadata": {},
   "source": [
    "c) Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20975a8-20b1-4cd8-8ffc-e9163bcbfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the entire validation set\n",
    "val_images, val_labels = next(val_generator)\n",
    "predictions = (model.predict(val_images) > 0.85).astype(\"int32\")\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(val_labels, predictions)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Damaged', 'Not Damaged'], yticklabels=['Damaged', 'Not Damaged'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02e950-9b6a-436b-93bc-7652b92a0e83",
   "metadata": {},
   "source": [
    "8. Model saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b6d18-54a0-4041-8232-111342103bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('damage_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9fb6d-ac11-4806-b620-6baa6038bb96",
   "metadata": {},
   "source": [
    "9. Model loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577101f2-b061-448a-b362-4af48564a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = load_model('damage_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9048d79-a334-482e-b027-b1c6d0244d87",
   "metadata": {},
   "source": [
    "10. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d181abe-df38-4a4c-b181-96108b2e42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and predict on a new image\n",
    "def predict_image(image_path):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize to model input size\n",
    "    img = img / 255.0  # Normalize pixel values\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(img)\n",
    "    \n",
    "    # Convert prediction to human-readable form\n",
    "    if prediction[0] > 0.85:\n",
    "        print(\"Prediction: The car is not damaged.\")\n",
    "    else:\n",
    "        print(\"Prediction: The car is damaged.\")\n",
    "\n",
    "# Predict on a random image\n",
    "image_path = r\"E:\\car_crash\\test\\test2.jpg\"\n",
    "predict_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225550c4-0e24-4365-bc3b-6017e33fd5c5",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf38d0b-1070-4cda-9026-11cb7a1a2b80",
   "metadata": {},
   "source": [
    "__STEP 3: DAMAGE LOCAlIZATION__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf79fd0-273b-42eb-a238-340c76427458",
   "metadata": {},
   "source": [
    "1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ef656-79ca-48c5-82d6-f94b47f31ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157d13a-901a-45ef-8e85-1e2a53e88bb6",
   "metadata": {},
   "source": [
    "2. Loading training & validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e30dae-4ba3-4cec-b8ab-61234253ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = r\"E:\\car_crash\\position\\training\"\n",
    "validation_data_dir = r\"E:\\car_crash\\position\\validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd28ba-740d-4784-998e-54e1867f0a6f",
   "metadata": {},
   "source": [
    "3. Data(image) preprocessing \n",
    "\n",
    "   a) creates data generators to preprocess images by rescaling pixel values , resizing them to the IMG_SIZE and assigning binary labels for classification.\n",
    "\n",
    "   b) prepares the data for a binary classification model, handling image loading, resizing, and normalization in batches (BATCH_SIZE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91cd03-4f6c-4834-b3fd-d264689cdebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Since we have 3 classes: front, side, back\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b0503-5286-48a4-b09e-bc61e85ae22b",
   "metadata": {},
   "source": [
    "4. CNN model\n",
    "   \n",
    "   a) using three convolutional layers with ReLU activation & passes it through fully connected layers for binary classification with a sigmoid output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7237b9b-6e8a-4657-ad5c-cf77fed8287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Add layers\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes (front, side, back)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fcd614-1e0f-402c-9be5-71d7608f7c8b",
   "metadata": {},
   "source": [
    "5. Model training \n",
    "\n",
    "   a) trains CNN model on training dataset\n",
    "   \n",
    "   b) tests the model on validation dataset\n",
    "   \n",
    "   c) runs for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9f4d1-7c39-43f0-8a05-832bcd4392dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,epochs=10,validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326e0ea-361b-4708-b4b2-b0d563837224",
   "metadata": {},
   "source": [
    "6. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c08ec9-b1ce-4c65-891b-ea784395fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd096ee-5581-44f0-adec-2511e0df00de",
   "metadata": {},
   "source": [
    "7. Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818411ae-9960-4bce-816a-4c0fa3936dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('location_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac780ce-e4d7-4d42-93a1-ecf002325cb3",
   "metadata": {},
   "source": [
    "8. Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b84e8-c3fb-40a8-829f-22d4e2036558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('location_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b416841-313b-4cd1-9c13-db9c2711f30a",
   "metadata": {},
   "source": [
    "9. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffb0db-6557-4b71-8a15-731ff3542531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Image dimensions\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Function to load and preprocess image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Rescale the image\n",
    "    return img_array\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"location_detection_model.h5\")\n",
    "\n",
    "# Predict damage location\n",
    "img_path = r\"E:\\car_crash\\test\\test1.jpg\"  # Replace with your image path\n",
    "img_array = load_and_preprocess_image(img_path)\n",
    "\n",
    "# Get prediction\n",
    "prediction = model.predict(img_array)\n",
    "\n",
    "# Decode prediction\n",
    "class_labels = ['front', 'side', 'back']\n",
    "predicted_class = np.argmax(prediction, axis=1)[0]  # Get the index of the highest probability\n",
    "predicted_label = class_labels[predicted_class]\n",
    "\n",
    "print(f\"The predicted damage location is: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac70f2f-e771-42b7-98f1-8060a6bc19e8",
   "metadata": {},
   "source": [
    "10. Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c7060-2d36-4f3a-87cc-43cec4421056",
   "metadata": {},
   "source": [
    "a) Training and Validation Accuracy and Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d82dae-563d-4ed6-b67d-dfb0aa5d29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b51b1-a6bc-4984-b97a-524cc36f7ff5",
   "metadata": {},
   "source": [
    "b) Confusion Matrix for Predictions on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddeec27-ed18-45e2-ae20-819f38d17ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Generate predictions on validation data\n",
    "Y_pred = model.predict(validation_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(validation_generator.classes, y_pred)\n",
    "class_names = ['front', 'side', 'back']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8ce46-4c5e-4138-9d56-3597d2192671",
   "metadata": {},
   "source": [
    "c) Visualization of Test Image and Predicted Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e319f1-08ab-4eea-9039-1d779694ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image with prediction\n",
    "img = Image.open(img_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Predicted Damage Location: {predicted_label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1ed63-1aaa-480b-adf5-096be0cd0b0c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32701f-1ecc-4909-baac-262472400cbc",
   "metadata": {},
   "source": [
    "__STEP 4: SEVERITY CHECK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc4f1a-ba9a-40d6-a6ff-4590419c1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the paths\n",
    "train_dir = r\"E:\\car_crash\\level\\training\"  # Replace with your training directory path\n",
    "val_dir = r\"E:\\car_crash\\level\\validation\"   # Replace with your validation directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8947a7-6776-49a5-89b1-eca3ca64ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "img_height, img_width = 150, 150  # Adjust based on your needs\n",
    "batch_size = 32\n",
    "epochs = 10  # You can adjust this based on your dataset\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43594ef1-715e-4144-a01d-29440d613050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Use 'categorical' for multi-class classification\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f382eaa-3df9-42ce-baaf-86b4ae3658d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # 3 output classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f38fa6-6fc7-42a2-9076-e8f0c002853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb382f-90f9-456d-b781-e04051b62447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d142a40-1a29-4669-a2ca-e53b9b6fe0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss\n",
    "def plot_history(history):\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a41a9-a70a-4a51-87a6-381b2844e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a file\n",
    "model.save('severity_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96996c99-a370-4ee1-a378-891aefc5bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category names corresponding to the class indices\n",
    "category_names = ['minor', 'moderate', 'severe']  # Adjust this based on your folder names\n",
    "\n",
    "# Function to predict category of a new image\n",
    "def predict_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    predicted_category = category_names[predicted_class_index]\n",
    "    \n",
    "    return predicted_category\n",
    "\n",
    "# Example prediction (replace with an actual image path)\n",
    "new_image_path = r\"E:\\car_crash\\test\\test3.jpg\"  # Replace with your image path\n",
    "predicted_category = predict_image(new_image_path)\n",
    "print(f'Predicted Category: {predicted_category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed47aa-83a2-40e7-b4fa-641b3caec2bb",
   "metadata": {},
   "source": [
    "STEP 5: PRICE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c9b1ba-e7ec-4324-b4a4-9f3101d3c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import plot_tree\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd0210d-ad35-402a-8844-38fa4de89ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'price_car_parts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_car_parts.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'price_car_parts.csv'"
     ]
    }
   ],
   "source": [
    " # Load the data\n",
    "data = pd.read_csv('price_car_parts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10229b65-066b-401b-a4e7-cdd59b9832ed",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed2a7b2-c7ac-4ba6-9efe-9ca89838a84c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select relevant features for predicting car part prices\u001b[39;00m\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngine HP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngine Cylinders\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighway MPG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity mpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPopularity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSRP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m data[features]\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSRP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Select relevant features for predicting car part prices\n",
    "features = ['Year', 'Engine HP', 'Engine Cylinders', 'highway MPG', 'city mpg', 'Popularity', 'MSRP']\n",
    "X = data[features]\n",
    "y = data['MSRP']  # Assuming MSRP as a proxy for car part prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e0dd21-82d6-45bb-90d7-4a505b1d1926",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Handle missing values\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mfillna(X\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c98e01b0-a061-405a-84d2-34c36965a9eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f0d715-e7c9-48fc-9e32-66c8ce55bb76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Scale the features\u001b[39;00m\n\u001b[1;32m      2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 3\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m      4\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6deb4081-0cad-407a-89ed-de428ea162f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Exploratory Data Analysis (EDA)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(X\u001b[38;5;241m.\u001b[39mcorr(), annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrelation Heatmap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c19178-989d-46a1-a013-e7b4c6568f03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model Implementation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dt_model \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m dt_model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Model Implementation\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b40a4b-5fe1-44f7-b9af-317b01eaefd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m dt_model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = dt_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff893abb-a0d3-4f27-a01a-a2de58bf25ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3321e495-048e-427c-9a91-7331d0367798",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This DecisionTreeRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize the decision tree\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plot_tree(dt_model, feature_names\u001b[38;5;241m=\u001b[39mfeatures, filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rounded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecision Tree Visualization\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/tree/_export.py:179\u001b[0m, in \u001b[0;36mplot_tree\u001b[0;34m(decision_tree, max_depth, feature_names, class_names, label, filled, impurity, node_ids, proportion, rounded, precision, ax, fontsize)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_tree\u001b[39m(\n\u001b[1;32m     79\u001b[0m     decision_tree,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m ):\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot a decision tree.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    The sample counts that are shown are weighted with any sample_weights that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    [...]\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     check_is_fitted(decision_tree)\n\u001b[1;32m    181\u001b[0m     exporter \u001b[38;5;241m=\u001b[39m _MPLTreeExporter(\n\u001b[1;32m    182\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m    183\u001b[0m         feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m         fontsize\u001b[38;5;241m=\u001b[39mfontsize,\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exporter\u001b[38;5;241m.\u001b[39mexport(decision_tree, ax\u001b[38;5;241m=\u001b[39max)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1386\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This DecisionTreeRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_model, feature_names=features, filled=True, rounded=True)\n",
    "plt.title('Decision Tree Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae10abc-a850-4a8d-913c-1a3ad8c80662",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This DecisionTreeRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Feature Importance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m: features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m: dt_model\u001b[38;5;241m.\u001b[39mfeature_importances_})\n\u001b[1;32m      3\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m feature_importance\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/tree/_classes.py:585\u001b[0m, in \u001b[0;36mBaseDecisionTree.feature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_importances_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the feature importances.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    The importance of a feature is computed as the (normalized) total\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03m        (Gini importance).\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mcompute_feature_importances()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1386\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This DecisionTreeRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({'feature': features, 'importance': dt_model.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387aaf1f-e8a5-4d1a-aebd-c67bf9c2b15b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeRegressor' object has no attribute 'tree_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SHAP Values\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mTreeExplainer(dt_model)\n\u001b[1;32m      3\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_test_scaled)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/shap/explainers/_tree.py:195\u001b[0m, in \u001b[0;36mTreeExplainer.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, link, linearize_link)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_perturbation \u001b[38;5;241m=\u001b[39m feature_perturbation\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m TreeEnsemble(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_missing, model_output)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m=\u001b[39m model_output\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/shap/explainers/_tree.py:821\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_value\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m safe_isinstance(\n\u001b[1;32m    814\u001b[0m     model,\n\u001b[1;32m    815\u001b[0m     [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    819\u001b[0m     ],\n\u001b[1;32m    820\u001b[0m ):\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_dtype \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees \u001b[38;5;241m=\u001b[39m [SingleTree(model\u001b[38;5;241m.\u001b[39mtree_, data\u001b[38;5;241m=\u001b[39mdata, data_missing\u001b[38;5;241m=\u001b[39mdata_missing)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeRegressor' object has no attribute 'tree_'"
     ]
    }
   ],
   "source": [
    "# SHAP Values\n",
    "explainer = shap.TreeExplainer(dt_model)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29de3fb-ab0d-4eab-b25f-cd62bfa6f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29efe9e4-a6a6-4e5e-a9d7-4e546ebd7d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50834525-c411-4f9c-b7b0-d091c04c832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "__PIPLELINE__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded67d94-dd2e-41f8-a0d1-c207fdfd79e0",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfc10d-641a-401f-b921-d18108021bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load all models\n",
    "car_detection_model = load_model('car_classification_model.h5')  # Step 1 model\n",
    "damage_detection_model = load_model('damage_detection_model.h5')  # Step 2 model\n",
    "location_detection_model = load_model('location_detection_model.h5')  # Step 3 model\n",
    "severity_model = load_model('severity_model.h5')  # CNN model for severity prediction\n",
    "\n",
    "# Class labels for predictions\n",
    "car_labels = ['not_car', 'car']\n",
    "damage_labels = ['not_damaged', 'damaged']\n",
    "location_labels = ['front', 'side', 'back']\n",
    "severity_labels = ['minor', 'moderate', 'severe']  # Adjust as per your dataset\n",
    "\n",
    "# Define thresholds for binary models\n",
    "car_threshold = 0.5\n",
    "damage_threshold = 0.5\n",
    "\n",
    "# Function to preprocess images\n",
    "def load_and_preprocess_image(img_path, img_width, img_height, flatten=False):\n",
    "    # Load and resize the image\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img) / 255.0  # Normalize\n",
    "\n",
    "    # Expand dimensions to match model input (batch size, width, height, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Flatten if the model expects a flat input\n",
    "    if flatten:\n",
    "        img_array = img_array.reshape(1, -1)\n",
    "\n",
    "    return img_array\n",
    "\n",
    "# Complete prediction pipeline\n",
    "def damage_detection_pipeline(img_path):\n",
    "    print(\"Starting prediction pipeline...\\n\")\n",
    "    \n",
    "    # Preprocessing for car detection\n",
    "    car_input_shape = car_detection_model.input_shape\n",
    "    img_width, img_height = car_input_shape[1], car_input_shape[2]\n",
    "    img_array = load_and_preprocess_image(img_path, img_width, img_height)\n",
    "\n",
    "    # Step 1: Car Detection\n",
    "    car_prediction = car_detection_model.predict(img_array)\n",
    "    car_predicted_confidence = car_prediction[0][0]  # Binary prediction confidence\n",
    "\n",
    "    if car_predicted_confidence >= car_threshold:\n",
    "        print(f\"Car Detection: The image contains a car with confidence: {car_predicted_confidence:.2f}\")\n",
    "\n",
    "        # Preprocessing for damage detection\n",
    "        damage_input_shape = damage_detection_model.input_shape\n",
    "        img_width, img_height = damage_input_shape[1], damage_input_shape[2]\n",
    "        flatten_damage_input = len(damage_input_shape) == 2\n",
    "        img_array = load_and_preprocess_image(img_path, img_width, img_height, flatten=flatten_damage_input)\n",
    "\n",
    "        # Step 2: Damage Detection\n",
    "        damage_prediction = damage_detection_model.predict(img_array)\n",
    "        damage_predicted_confidence = damage_prediction[0][0]  # Binary prediction confidence\n",
    "\n",
    "        if damage_predicted_confidence >= damage_threshold:\n",
    "            print(f\"Damage Detection: The car is damaged with confidence: {damage_predicted_confidence:.2f}\")\n",
    "\n",
    "            # Preprocessing for location detection\n",
    "            location_input_shape = location_detection_model.input_shape\n",
    "            img_width, img_height = location_input_shape[1], location_input_shape[2]\n",
    "            flatten_location_input = len(location_input_shape) == 2\n",
    "            img_array = load_and_preprocess_image(img_path, img_width, img_height, flatten=flatten_location_input)\n",
    "\n",
    "            # Step 3: Location Detection\n",
    "            location_prediction = location_detection_model.predict(img_array)\n",
    "            location_predicted_class = np.argmax(location_prediction, axis=1)[0]\n",
    "            location_confidence = location_prediction[0][location_predicted_class]\n",
    "\n",
    "            location_result = f\"Damage is at the {location_labels[location_predicted_class]} with confidence: {location_confidence:.2f}\"\n",
    "            print(location_result)\n",
    "\n",
    "            # Preprocessing for severity detection (same image)\n",
    "            severity_input_shape = severity_model.input_shape\n",
    "            img_width, img_height = severity_input_shape[1], severity_input_shape[2]\n",
    "            img_array = load_and_preprocess_image(img_path, img_width, img_height)\n",
    "\n",
    "            # Step 4: Severity Detection\n",
    "            severity_prediction = severity_model.predict(img_array)\n",
    "            severity_class = np.argmax(severity_prediction, axis=1)[0]\n",
    "            severity_confidence = severity_prediction[0][severity_class]\n",
    "\n",
    "            severity_result = f\"Damage severity is {severity_labels[severity_class]} with confidence: {severity_confidence:.2f}\"\n",
    "            print(severity_result)\n",
    "\n",
    "        else:\n",
    "            print(f\"The car is not damaged with confidence: {1 - damage_predicted_confidence:.2f}\")\n",
    "    else:\n",
    "        print(f\"The image does not contain a car with confidence: {1 - car_predicted_confidence:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "img_path = r\"E:\\car_crash\\test\\test2.jpg\"  # Replace with the actual image path\n",
    "damage_detection_pipeline(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406233dd-7ae9-473d-bc64-268c8f9c4494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a3d86-b080-4222-83c5-d6c4cfb05f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
